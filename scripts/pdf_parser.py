import os
import re
import boto3
from pypdf import PdfReader

# Load configuration from environment variables or a config file
AWS_ACCESS_KEY_ID = "AKIA6ODU6VDB2NXG2KVA"
AWS_SECRET_ACCESS_KEY = "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"
AWS_S3_REGION_NAME = "us-west-2"
AWS_STORAGE_BUCKET_NAME = "research-llm-pdfs"
MEDIA_ROOT = "/tmp"  # Temporary storage for processing files

# Global counter for paperID
paper_id_counter = 1

# Dictionary to store the cleaned_filename as key and [S3 link, paperID] as value
filename_metadata = {}

def s3_upload(file):
    """
    Uploads a file to S3 and returns the public URL.
    """
    if 1==0:
        try:
            file_path = os.path.join(MEDIA_ROOT, file.name)
            with open(file_path, "wb") as f:
                f.write(file.read())

            s3 = boto3.resource(
                "s3",
                aws_access_key_id=AWS_ACCESS_KEY_ID,
                aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
                region_name=AWS_S3_REGION_NAME,
            )
            
            s3.Bucket(AWS_STORAGE_BUCKET_NAME).upload_file(
                file_path,
                "infer-soft/" + file.name,
            )
            
            file_url = f"https://{AWS_STORAGE_BUCKET_NAME}.s3.{AWS_S3_REGION_NAME}.amazonaws.com/infer-soft/{file.name}"
            os.remove(file_path)

            return file_url
        except Exception as e:
            print(f"Error uploading file to S3: {e}")
            return ""
    return ""

def parse_pdf_2(pdf_path):
    reader = PdfReader(pdf_path)
    number_of_pages = len(reader.pages)
    text = ""
    for i in range(number_of_pages):
        page = reader.pages[i]
        text += page.extract_text()
    return text

def save_text_to_file(text, output_path):
    with open(output_path, 'w', encoding='utf-8') as file:
        file.write(text)

def process_pdfs_in_folder(folder_path):
    paper_id_counter=1
    output_path = '/Users/shreyasnyu/Documents/projects/cloud/parsed'
    os.makedirs(output_path, exist_ok=True)
    
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            try:
                pdf_path = os.path.join(folder_path, filename)
                parsed_text = parse_pdf_2(pdf_path)

                # Clean special characters from filename
                cleaned_filename = re.sub(r'[^A-Za-z0-9]', '_', os.path.splitext(filename)[0])
                
                # Use the s3_upload function to upload the file and get the S3 URL
                with open(pdf_path, 'rb') as file:
                    # file.name = filename  # Simulate a file object with a name attribute
                    s3_url = s3_upload(file) + "something"

                if s3_url:
                    # Store the metadata
                    filename_metadata[cleaned_filename] = [paper_id_counter, s3_url, filename]
                    paper_id_counter += 1
    
                # Save parsed text to file
                output_text_file = os.path.join(output_path, f"{cleaned_filename}.txt")
                save_text_to_file(parsed_text, output_text_file)
                print(f"Parsed text has been saved to {output_text_file}")
                print(f"Metadata for {cleaned_filename}: {filename_metadata[cleaned_filename]}")
                break
            except Exception as e:
                print(f"Error processing {filename}: {e}")

# Usage
folder_path = '/Users/shreyasnyu/Documents/projects/cloud/datasets2'
if os.path.exists(folder_path):
    process_pdfs_in_folder(folder_path)
    print("Final Metadata Dictionary:", filename_metadata)
else:
    print(f"The folder path '{folder_path}' does not exist.")
